{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dabe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\anmol\\anaconda3\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from glob import glob\n",
    "import os\n",
    "transform = transforms.ToTensor()\n",
    "import lpips\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import scipy.io\n",
    "loss_fn_alex = lpips.LPIPS(net='alex') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d78708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84607fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw = scipy.io.loadmat(\"hw5.mat\")\n",
    "blur_dmos = np.array(hw['blur_dmos']).ravel()\n",
    "blur_orgs = np.array(hw['blur_orgs']).ravel()\n",
    "refnames_blur = np.array(hw['refnames_blur']).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "001a3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_handling(blur_dmos , blur_orgs , refnames_blur ):\n",
    " reference_img = []\n",
    " distortion_img = []\n",
    " index = 0\n",
    " DMOS = []\n",
    " files_path =sorted(glob(\"gblur/*.bmp\"), key = lambda x   : int(os.path.splitext(os.path.basename(x))[0][3:]))\n",
    " for f in files_path:\n",
    "    if(blur_orgs[index]==0):\n",
    "      ref_img = refnames_blur[index][0]\n",
    "      ref_path = Path('refimgs/'+ str(ref_img))\n",
    "      ref_img = io.imread(ref_path)\n",
    "      dis_img = io.imread(f)\n",
    "      reference_img.append(rgb2gray(ref_img))\n",
    "      distortion_img.append(rgb2gray(dis_img))\n",
    "      DMOS.append(blur_dmos[index])\n",
    "      index +=1      \n",
    "    else :\n",
    "      index += 1\n",
    "      continue\n",
    " return np.array(reference_img) , np.array(distortion_img) , np.array(DMOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b56c2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensor_detach(LPIPS_Ind):\n",
    " st_l =[]\n",
    " for i in range(145):\n",
    "        st=str(LPIPS_Ind[i][0][0][0][0])\n",
    "        st= st[7:13]\n",
    "        st_l.append(float(st))\n",
    " LPIPS_Ind = np.array(st_l)\n",
    " return LPIPS_Ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c52f72cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-95b8a5907cf4>:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(reference_img) , np.array(distortion_img) , np.array(DMOS)\n"
     ]
    }
   ],
   "source": [
    "reference_imgs , distortion_imgs , DMOS_vals = file_handling(blur_dmos , blur_orgs , refnames_blur )\n",
    "MSE =        []\n",
    "SSIM_Index = []\n",
    "LPIPS_Ind =  []\n",
    "for k in range(145): \n",
    "      img1 = reference_imgs[k]\n",
    "      img2 = distortion_imgs[k]\n",
    "      mean_square_error = mean_squared_error(img1 , img2)\n",
    "      SSIM              = ssim(img1 , img2, data_range=255)\n",
    "      img1 = img1.astype(np.float32)\n",
    "      img2 = img2.astype(np.float32)\n",
    "      LPIPS = loss_fn_alex(transform(img1), transform(img2))\n",
    "      MSE.append(mean_square_error)\n",
    "      SSIM_Index.append(SSIM)\n",
    "      LPIPS_Ind.append(LPIPS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8e9f004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores\n",
      "0.7833175877814516\n",
      "-0.8552157140607777\n",
      "0.9599234763240657\n"
     ]
    }
   ],
   "source": [
    "LPIPS = Tensor_detach(LPIPS_Ind)\n",
    "print(\"Scores\")\n",
    "coeff1 , p1 = stats.spearmanr(np.array(DMOS_vals), np.array(MSE))\n",
    "coeff2 , p2 = stats.spearmanr(np.array(DMOS_vals), np.array(SSIM_Index))\n",
    "coeff3 , p3 = stats.spearmanr(np.array(DMOS_vals), LPIPS)\n",
    "print(coeff1)\n",
    "print(coeff2)\n",
    "print(coeff3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352791e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
